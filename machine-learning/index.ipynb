{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "### Machine Learning Reading Group,  The University of Melbourne\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Dr Peter Cudmore  \n",
    "Systems Biology Lab  \n",
    "The University of Melbourne\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Slides at:  \n",
    "https://github.com/peter-cudmore/seminars/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNN's) are neural networks with'memory'.  \n",
    "\n",
    "Useful for:\n",
    "- Natural Language Processing\n",
    "- Signal Reconstructing\n",
    "- Motion Tracking/Control\n",
    "\n",
    "Recurrent Neural Networks good for *sequential data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feedback in RNN's\n",
    "\n",
    "Memory is usually implemented by introduction *feedback loops* into the neural network.  \n",
    "\n",
    "Feedback can be introduced at different places in the network; at the output stage, between layers, or at the individual artifical neuron level.\n",
    "\n",
    "\n",
    "Introducing feedback has some consequences:\n",
    "\n",
    "- Topolgy is very important.\n",
    "- 'Infinite Impulse Response' implies truncated training\n",
    "- Gradient magnitude issues are very common.\n",
    "- Stability, strange attractors, chaos, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Draw an example of a neural network and contrast agianst one with a feedback loop (ie, RNN).\n",
    "- Draw output feedback topology, layer feedback, node feedback \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNNs and Signal Processing\n",
    "\n",
    "\n",
    "Suppose:\n",
    "- We have partial and/or noisy observations $x(t)$ of some sequential process \n",
    "- which is asumed to evolve on a space $z$ according to some evolution rule $f$,\n",
    "- and is measured to make a prediciton or desicion $\\hat{y}(t)$.\n",
    "\n",
    "If we were doing control systems, might write this as  \n",
    "\n",
    "$$\n",
    "\\dot{z} = f(z,x)\\qquad \\hat{y} = g(z,x)\n",
    "$$  \n",
    "to predict $\\hat{y}$ from some sequence of values of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Useful to refer to linear control systems $\\dot{z} = Az + Bx$ and $\\hat{y} = Cz + Dx$ \n",
    "- $z$ is always going to refer to interal state\n",
    "- $x$ is always going to be input data\n",
    "- $\\hat{y}$ is always going to be RNN output\n",
    "- $y$ is always going to be ground truth, when it is know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RNNs and Digital Signal Processing\n",
    "Consider\n",
    "$$\\dot{z} = f(z,x)\\qquad \\hat{y} = g(z,x).$$\n",
    "\n",
    "A RNN can be intepreted as:\n",
    "- Applying some discritization scheme to the evolution on some sufficiently large state space such that $z_n = F(z_{n-1},x_n)$.\n",
    "- Applying the *Universal Function Theorem*  to approximate the resulting function $F$ with a feed-forward neural network.\n",
    "- Conceptually discritizing and splitting $g$ into post-processing (for example, soft-max to transform the output layer data into a pdf) and 'everything else' (which then assimmilated into the network)\n",
    "\n",
    "RNN in general form::\n",
    "$$\n",
    "z_n = F(z_{n-1}, x_n; \\mathbb{\\theta}),\\qquad \\hat{y}_n = M(z_n, x_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Observations:\n",
    "- Different topolgoes correspond to different discritization schemes\n",
    "- RNN's are _compositional_ (more on this soon)\n",
    "- RNN's are Iterated Funciton Systems: class of functions know to generate fractals etc.\n",
    "- The diff-eq representation makes it clear that we need some $z(0)$ (we usually assume to be zero for training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example RNN\n",
    "A simple example of a RNN with one hidden layer is\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "z_n &=& b + Wh_{n-1} + Ux_n\\\\\n",
    "h_{n} &=& \\tanh(z_n)\\\\\n",
    "o_n &=& c +Vh_n\\\\\n",
    "\\hat{y}_n &=& \\text{softmax}(o_n)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "In this case\n",
    "$$ z_n = F(z_{n-1}, x_n) = b + W\\tanh(z_{n-1}) + Ux_n$$\n",
    "and\n",
    "\n",
    "$$ G(z_n,x_n) = \\text{softmax}\\left(c + V\\tanh(z_n)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here:\n",
    " - parameters are $\\theta = \\{b, W, U, c, V\\}$,\n",
    " - $z_n$ is the state at step $n$,\n",
    " - $h_n$ is the hidden layer at step $n$,\n",
    " - $o_n$ is the output layer at step $n$,\n",
    " - $\\tanh$ is the elementwise tanh\n",
    " - $\\text{softmax}(o_n) = \\exp(o_n)/\\sum(\\exp(o_n))$ is the softmax, or normalised  elementwise exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Composition and Unfolding \n",
    "\n",
    "Instead of thinking of a RNN acting in an iterative sense, it is often useful to 'unfold' the neural network, that is think of it as a map from $G: X^N \\rightarrow Y^N$, where $x_1,x_2\\in X$ (for example, vectors EEG data at time $t$) and $y_1\\in Y$ (the corresponding decision/ouptut). \n",
    "\n",
    "Recall:\n",
    "$$z_n = F(z_{n-1}, x_n; \\mathbb{\\theta}),\\qquad y_n = M(z_n, x_n)$$\n",
    "then it follows that\n",
    "$$ \\hat{y}_1 = M(F(z_0,x_1), x_1),\\quad \\hat{y}_2 = M(F(F(z_0,x_1),x_2),x_2), \\quad \\ldots$$   \n",
    "Lets define $M_n(z) := M(z,x_n)$ and $F_n:= F(z,x_n)$ then it follows that\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\hat{y}_1 &=& G_1(z_0, x_1) = M_1\\circ F_1(z_0) = M_1(F_1(z_0)) \\\\\n",
    " \\hat{y}_2 &=& G_2(z_0, x_1, x_2) = M_2 \\circ F_2\\circ F_1(z_0) \\\\\n",
    "& &\\qquad\\vdots \\\\\n",
    " \\hat{y}_N  &=&G_N(z_0,x_1,x_2,\\ldots,x_N;\n",
    "\\mathbb{\\theta}) =  M_N\\circ F_N\\circ F_{N-1}\\circ\\cdots\\circ F_1 (z_0)\\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Draw \n",
    "![Unrolled RNN](images/UnrolledRNN.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advantages of Unfolding\n",
    "\n",
    "1. Unfolding makes the input-ouput conditioning explicit\n",
    "2. Writing out the composition sequences makes it clear how to do gradient descent. \n",
    "\n",
    "Applying backprop to the unfolded graph is known as _Back-propogation through time._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training A RNN with Backpropogation through time\n",
    "\n",
    "Requires:\n",
    "- A sequence size $N$, which determines how far to unroll the graph.\n",
    "- Test input sequence $\\{x_n\\}_{n=1}^K$ and corresponding true outputs $\\{y_n\\}_{n=1}^K$ for some $K = N + k$ where $k$ is the batch size.\n",
    "- A loss function $L$.\n",
    "- An initial guess for the state: $z_0 =0$.\n",
    "- The RNN $F(z,x;\\theta)$.\n",
    "- The graph unfolding $G_N(x_1,x_2,\\ldots;\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Algorithm:\n",
    "\n",
    "    while training:\n",
    "        z = 0\n",
    "        for step from 0 to k:\n",
    "            test_sequence = x[step:step+N]\n",
    "            y_hat = G_N(z, test_sequence, theta)\n",
    "            error = y - y_hat\n",
    "            theta = backprop(G, L, theta, error). \n",
    "            z = F(z, x[step+ N], theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Computation time is $O(N)$\n",
    "- Memory is $O(N)$\n",
    "- Can have problems with local mins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example RNN redux\n",
    "A simple example of a RNN with one hidden layer is\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "z_n &=& b + Wh_{n-1} + Ux_n\\\\\n",
    "h_{n} &=& \\tanh(z_n)\\\\\n",
    "o_n &=& c +Vh_n\\\\\n",
    "\\hat{y}_n &=& \\text{softmax}(o_n)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "In this case\n",
    "$$ z_n = F(z_{n-1}, x_n) = b + W\\tanh(z_{n-1}) + Ux_n$$\n",
    "and\n",
    "\n",
    "$$ G(z_n,x_n) = \\text{softmax}\\left(c + V\\tanh(z_n)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For a sequence of lenght $N$, $\\nabla_{o_N}L$ is known.  \n",
    "We compute\n",
    "$$\\nabla_{h_N} L = V^T \\nabla_{o_N}L$$\n",
    "and for any $0<n<N$ \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\nabla_{h_{n-1}}L &=& \\left(\\frac{\\partial h_n}{\\partial n_{n-1}}\\right)^T(\\nabla_{h_n}L) + \\left(\n",
    "\\frac{\\partial o_{n-1}}{\\partial h_{n-1}}\\right)^T\\nabla_{o_n}L\\\\\n",
    "&=& W^T\\text{diag}(1-h_n^2)(\\nabla_{h_n}L) + V^T \\nabla_{o_n}L\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The $n-1$th step can be computed from the $n$th step.  \n",
    "This relies on the fact that $$\n",
    "f(x) = \\tanh(z) \\implies \\frac{\\partial f}{\\partial x} = 1 - \\tanh{x}^2 =  1 - f(x)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Teacher Forcing\n",
    "\n",
    "When the output is fedback $z_n = \\hat{y}_n$, we can use what is called 'teacher forcing' to update the state.\n",
    "\n",
    "This avoids BPTT, but can cause problems if trained in open loop, then deployed in closed loop mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Algorithm:\n",
    "    \n",
    "    z = 0\n",
    "    for (x, y) in training set:\n",
    "        y_hat = F(z, x, theta)\n",
    "        error = y - y_hat\n",
    "        theta = backprop(F, L, theta, error)\n",
    "        z = y \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN's, Signals and computation\n",
    "\n",
    "Some observations:\n",
    "- In the absence of input $x = 0$, the RNN obeys the 'Markov Propery' (next state only depends on the current state) and hence generates a stationary process $y_1,y_2,\\ldots y_n$.\n",
    "- Hence, RNN are causal (but there are some ways around this)!\n",
    "- In the presence of input the process $y_1,y_2,\\ldots$ is no longer stationary (courtesy of the graph $G$) and hence may have long-range dependence encoded in the interal state $z$\n",
    "- RNN's are Turing complete (see Siegelmann and Sontag https://doi.org/10.1006/jcss.1995.1013 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Recurrent Neural Networks (RNN's) are neural networks with a feedback loop. \n",
    "- RNN's are good for sequential data, and NLP in particular.\n",
    "- RNN's have analogies in discrete nonlinear contorl systems\n",
    "- Training a RNN usually involves Backpropogation through time, by training on fixed length sequences.\n",
    "- This comes at the expense of parallelisation and may get stuck in local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thanks:\n",
    "- Prof. Jonathan Manton \n",
    "- Selwyn Gomes\n",
    "- The machine Learning reading group at UniMelb\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center> https://github.com/peter-cudmore/seminars/ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
