{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "### Machine Learning Reading Group, \n",
    "### The University of Melbourne\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Dr Peter Cudmore  \n",
    "Systems Biology Lab  \n",
    "The University of Melbourne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Recurrent Neural Networks (RNN's) are neural networks with'memory'.  \n",
    "\n",
    "Useful for:\n",
    "- Natural Language Processing\n",
    "- Signal Reconstructing\n",
    "- Motion Tracking/Control\n",
    "\n",
    "Recurrent Neural Networks for *sequential data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feedback in RNN's\n",
    "\n",
    "Memory is usually implemented by introduction *feedback loops* into the neural network.  \n",
    "\n",
    "Feedback can be introduced at different places in the network; at the output stage, between layers, or at the individual artifical neuron level.\n",
    "\n",
    "\n",
    "Introducing feedback has some consequences:\n",
    "\n",
    "- Topolgy is very important.\n",
    "- 'Infinite Impulse Response' implies truncated training\n",
    "- Gradient magnitude issues are very common.\n",
    "- Stability, strange attractors, chaos, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Draw an example of a neural network and contrast agianst one with a feedback loop (ie, RNN).\n",
    "- Draw output feedback topology, layer feedback, node feedback \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNNs and Signal Processing\n",
    "\n",
    "\n",
    "Suppose:\n",
    "- We have partial and/or noisy observations $x(t)$ of some sequential process \n",
    "- which is asumed to evolve on a space $z$ according to some evolution rule $f$,\n",
    "- and is measured to make a prediciton or desicion $y(t)$.\n",
    "\n",
    "If we were doing control systems, might write this as  \n",
    "\n",
    "$$\n",
    "\\dot{z} = f(z,x)\\qquad y = g(z,x)\n",
    "$$  \n",
    "to predict $y$ from some sequence of values of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RNNs and Digital Signal Processing\n",
    "Consider\n",
    "$$\\dot{z} = f(z,x)\\qquad y = g(z,x).$$\n",
    "\n",
    "A RNN can be intepreted as:\n",
    "- Applying some discritization scheme to the evolution on some sufficiently large state space such that $z_n = F(z_{n-1},x_n)$.\n",
    "- Applying the *Universal Function Theorem*  to approximate the resulting function $F$ with a feed-forward neural network.\n",
    "- Conceptually discritizing and splitting $g$ into post-processing (for example, soft-max to transform the output layer data into a pdf) and 'everything else' (which then assimmilated into the network)\n",
    "\n",
    "RNN in general form::\n",
    "$$\n",
    "z_n = F(z_{n-1}, x_n; \\mathbb{\\theta}),\\qquad y_n = M(z_n, x_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Observations:\n",
    "- RNN's are _compositional_ (more on this soon)\n",
    "- RNN's are Iterated Funciton Systems: class of functions know to generate fractals etc.\n",
    "- The diff-eq representation makes it clear that we need some $z(0)$ (we usually assume to be zero for training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Composition and Unfolding \n",
    "\n",
    "Instead of thinking of a RNN acting in an iterative sense, it is often useful to 'unfold' the neural network, that is think of it as a map from $G: X^N \\rightarrow Y^N$, where $x_1,x_2\\in X$ (for example, vectors EEG data at time $t$) and $y_1\\in Y$ (the corresponding decision/ouptut). \n",
    "\n",
    "Recall:\n",
    "$$z_n = F(z_{n-1}, x_n; \\mathbb{\\theta}),\\qquad y_n = M(z_n, x_n)$$\n",
    "then it follows that\n",
    "$$ y_1 = M(F(z_0,x_1), x_1),y_2 = M(F(F(z_0,x_1),x_2),x_2), \\ldots$$   \n",
    "Lets define $M_n(z) := M(z,x_n)$ and $F_n:= F(z,x_n)$ then it follows that\n",
    "$$ y_1 = M_1\\circ F_1(z_0) = M_1(F_1(z_0)) = G_1(z_0, x_1)$$\n",
    "$$ y_2 = M_2 \\circ F_2\\circ F_1(z_0) = G_2(z_0, x_1, x_2)$$\n",
    "$$\\vdots$$\n",
    "$$ y_N  = M_N\\circ F_N\\circ F_{N-1}\\circ\\cdots\\circ F_1 (z_0) = G_N(z_0,x_1,x_2,\\ldots,x_N;\n",
    "\\mathbb{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Unrolled RNN](images/UnrolledRNN.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advantages of Unfolding\n",
    "\n",
    "1. Unfolding makes the input-ouput conditioning explicit\n",
    "2. Writing out the composition sequences makes it clear how to do gradient descent. \n",
    "\n",
    "Applying backprop to the unfolded graph is known as _Back-propogation through time._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training A RNN with Backpropogation through time\n",
    "\n",
    "Requires:\n",
    "- A sequence size $N$, which determines how far to unroll the graph.\n",
    "- Test input sequence $\\{x_n\\}_{n=1}^K$ and corresponding true outputs $\\{\\hat{y_n}\\}_{n=1}^K$ for some $K = N + k$ where $k$ is the batch size.\n",
    "- A loss function $L$.\n",
    "- An initial guess for the state: $z_0 =0$.\n",
    "- The RNN $F(z,x;\\theta)$.\n",
    "- The graph unfolding $G_N(x_1,x_2,\\ldots;\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Algorithm::\n",
    "\n",
    "    while training:\n",
    "        z = 0\n",
    "        for step from 0 to k:\n",
    "            test_sequence = x[step:step+N]\n",
    "            y = G_N(test_sequence, theta)\n",
    "            error = y_hat - y\n",
    "            theta = backprop error across G. \n",
    "            z = F(z, x[step+ N], theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN's and statistical signal processing\n",
    "\n",
    "Some observations:\n",
    "- In the absence of input $x = 0$, the RNN obeys the 'Markov Propery' (next state only depends on the current state) and hence generates a stationary process $y_1,y_2,\\ldots y_n$.\n",
    "- Hence, RNN are causal (but there are some ways around this)!\n",
    "- In the presence of input the process $y_1,y_2,\\ldots$ is no longer stationary (courtesy of the graph $G$) and hence may have long-range dependence encoded in the interal state $z$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
